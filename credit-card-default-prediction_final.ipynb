{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# numpy and pandas imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# ML Libraries for scikit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,\\\n",
    "                            precision_recall_curve,precision_score,recall_score,roc_auc_score,roc_curve,\\\n",
    "                            matthews_corrcoef, f1_score, make_scorer, auc\n",
    "from scipy.stats import skew\n",
    "\n",
    "#  Recursive Feature Elimination\n",
    "from sklearn.feature_selection import RFE\n",
    "        \n",
    "# Python imports\n",
    "from math import log, sqrt\n",
    "import re\n",
    "import collections\n",
    "import warnings\n",
    "\n",
    "# decision tree visualization related imports\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_original = pd.read_csv('UCI_Credit_Card.csv',index_col='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = df_original.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About DataSet and Some dataset column naming standardizations\n",
    "+ Column naming standardization (Replacing pay_0 with pay_1 to make it standard with other columns)\n",
    "+ lower case names for columns (referring to them using column name would make it easy)\n",
    "+ Dataset - domain information and data type information, interpretations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lowercase the column name, and rename the column names when required, In particular, remarkably this dataset misses a colum  PAY_1. In the analysis here below we assume that PAY_0 is actually pay_1, to be consider the repayment of the month prior to the month where we calculate the defaulting (which is October 2005, in this particular dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.columns = [x.lower() for x in df.columns]\n",
    "df = df.rename(columns={\"pay_0\": \"pay_1\",\"default payment next month\":\"default_pay\"})\n",
    "cols = df.columns.values\n",
    "col_names = [str(s) for s in cols]\n",
    "# col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 30000 entries, 1 to 30000\n",
      "Data columns (total 24 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   limit_bal                   30000 non-null  float64\n",
      " 1   sex                         30000 non-null  int64  \n",
      " 2   education                   30000 non-null  int64  \n",
      " 3   marriage                    30000 non-null  int64  \n",
      " 4   age                         30000 non-null  int64  \n",
      " 5   pay_1                       30000 non-null  int64  \n",
      " 6   pay_2                       30000 non-null  int64  \n",
      " 7   pay_3                       30000 non-null  int64  \n",
      " 8   pay_4                       30000 non-null  int64  \n",
      " 9   pay_5                       30000 non-null  int64  \n",
      " 10  pay_6                       30000 non-null  int64  \n",
      " 11  bill_amt1                   30000 non-null  float64\n",
      " 12  bill_amt2                   30000 non-null  float64\n",
      " 13  bill_amt3                   30000 non-null  float64\n",
      " 14  bill_amt4                   30000 non-null  float64\n",
      " 15  bill_amt5                   30000 non-null  float64\n",
      " 16  bill_amt6                   30000 non-null  float64\n",
      " 17  pay_amt1                    30000 non-null  float64\n",
      " 18  pay_amt2                    30000 non-null  float64\n",
      " 19  pay_amt3                    30000 non-null  float64\n",
      " 20  pay_amt4                    30000 non-null  float64\n",
      " 21  pay_amt5                    30000 non-null  float64\n",
      " 22  pay_amt6                    30000 non-null  float64\n",
      " 23  default.payment.next.month  30000 non-null  int64  \n",
      "dtypes: float64(13), int64(11)\n",
      "memory usage: 5.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:black; font-family: 'calibri'; font-size: 1.2em;\">As you can see from the above information, the dataset does not look to have missing values.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanatory variables:  23\n",
      "Number of Observations: 30000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'default_pay'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'default_pay'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplanatory variables:  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of Observations: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m----> 4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault_pay\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdefault_pay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# create a 'target' column for our own convenience\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget variable:       \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault payment next month\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault_pay\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'default_pay'"
     ]
    }
   ],
   "source": [
    "print(\"Explanatory variables:  {}\".format(len(df.columns)-1))\n",
    "print(\"Number of Observations: {}\".format(df.shape[0]))\n",
    "\n",
    "df['default_pay'] = df['default_pay'].astype('category')\n",
    "\n",
    "# create a 'target' column for our own convenience\n",
    "print(\"Target variable:       '{}' -> '{}'\".format('default payment next month', 'default_pay'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSet description\n",
    "<span style=\"color:black; font-family: 'calibri'; font-size: 1.2em;\">This study leverages following \n",
    "+ <b>Dependent Variable</b>  : default payment (Yes = 1, No = 0), as the response variable\n",
    "+ <b>Independent Variable</b>: 23 variables as explanatory variables and they are numeric. These are as follows : \n",
    "    \n",
    "```\n",
    "   ----------------------------------------------------------------------------------------------------------------------\n",
    "   Name                   Numeric / Categorical          Explantion        \n",
    "   ----------------------------------------------------------------------------------------------------------------------\n",
    "   limit_bal              Numeric                        Amount of the given credit (NT dollar): \n",
    "                                                         it includes both the individual consumer credit \n",
    "                                                         and his/her family (supplementary) credit.\n",
    "   \n",
    "   sex                    Categorical                    Gender \n",
    "                                                         (1 = male; 2 = female)\n",
    "   \n",
    "   education              Categorical                  Education\n",
    "                                                       (1 = graduate school; 2 = university; 3 = high school; 4 = others)\n",
    "   \n",
    "   marriage               Categorical                    Marital status \n",
    "                                                         (1 = married; 2 = single; 3 = others)\n",
    "   \n",
    "   age                    Numeric                        Age (years)\n",
    "   \n",
    "   pay_0 - pay_6          Numeric                        History of past payment. Past monthly payment records \n",
    "                                                         From April to September, 2005 as follows:\n",
    "                        \n",
    "                                                         pay_0 = the repayment status in September, 2005\n",
    "                                                         pay_2 = the repayment status in August, 2005\n",
    "                                                         ...\n",
    "                                                         pay_6 = the repayment status in April, 2005 \n",
    "                        \n",
    "                                                         The measurement scale for the repayment status is: \n",
    "                                                         -1 = pay duly; \n",
    "                                                         1 = payment delay for one month \n",
    "                                                         2 = payment delay for two months\n",
    "                                                         ...\n",
    "                                                         8 = payment delay for eight months \n",
    "                                                         9 = payment delay for nine months and above\n",
    "                        \n",
    "   bill_amt1-bill_amt5    Numeric                        Amount of bill statement (NT dollar). \n",
    "                                                         bill_amt1 = amount of bill statement in September, 2005 \n",
    "                                                         bill_amt2 = amount of bill statement in August, 2005\n",
    "                                                         ...\n",
    "                                                         bill_amt6= amount of bill statement in April, 2005 \n",
    "                        \n",
    "   pay_amt1-pay_amt6      Numeric                        Amount of previous payment (NT dollar)\n",
    "                                                         pay_amt1 = amount paid in September, 2005\n",
    "                                                         pay_amt2 = amount paid in August, 2005\n",
    "                                                         ...\n",
    "                                                         pay_amt6 = amount paid in April, 2005 \n",
    "   ----------------------------------------------------------------------------------------------------------------------\n",
    "```\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some initializations and reusable methods block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all the mertics at one place for all the models used during training. This will help in comarison of these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# commonly used variables\n",
    "pay_status_columns = []\n",
    "for x in range(1,7): pay_status_columns.append(\"pay_\"+ str(x))\n",
    "    \n",
    "bill_amt_columns = []\n",
    "for x in range(1,7): bill_amt_columns.append(\"bill_amt\"+ str(x))\n",
    "    \n",
    "pay_amt_columns = []\n",
    "for x in range(1,7): pay_amt_columns.append(\"pay_amt\"+ str(x))\n",
    "\n",
    "\n",
    "# initialization block for ML\n",
    "perf_metrics = ['Model','Model_Desc','TP','FP','FN','TN','AUC',\\\n",
    "                'Accuracy_Score','Precision','Recall',\\\n",
    "                'F1_Score', 'Matt_Coeff']\n",
    "df_all_models = pd.DataFrame(columns=perf_metrics)\n",
    "perf_metric_per_model = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This is only required during model building exercise\n",
    "# for model evaluation\n",
    "# Get the performance metrics for the models\n",
    "def save_performance_metrics(model, model_name, model_dsc, X_test, y_test, X_train=None, y_train=None):\n",
    "    # nested inner method for adding performance metric\n",
    "    def add_perf_metric_to_compare(perf_metric_per_model):\n",
    "        global df_all_models\n",
    "        model_name = perf_metric_per_model['Model']\n",
    "        # if model data already exists then drop those rows\n",
    "        if df_all_models[df_all_models['Model'] == model_name].shape[0] >= 1:\n",
    "            ind = df_all_models[df_all_models['Model']== model_name].index\n",
    "            df_all_models.drop(ind, inplace=True)\n",
    "        s1 = pd.Series(perf_metric_per_model, perf_metric_per_model.keys())\n",
    "        df_all_models = df_all_models.append(s1, ignore_index=True)    \n",
    "    \n",
    "    perf_metric_per_model = {}\n",
    "#     print \"Evaluating performance for {} description is {}\"\\\n",
    "#                                         .format(model_name,model_description)\n",
    "    perf_metric_per_model['Model'] = model_name\n",
    "    perf_metric_per_model['Model_Desc'] = model_dsc\n",
    "    y_pred = model.predict(X_test)\n",
    "    conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "    tn, fp, fn, tp = conf_mat.ravel()\n",
    "    perf_metric_per_model['TP'], perf_metric_per_model['FP'] = tp, fp\n",
    "    perf_metric_per_model['FN'], perf_metric_per_model['TN'] = fn, tn\n",
    "    perf_metric_per_model['AP'], perf_metric_per_model['AN'] = tp+fn, fp+tn\n",
    "    perf_metric_per_model['AUC'] = roc_auc_score(y_test, y_pred)\n",
    "    perf_metric_per_model['F1_Score'] = f1_score(y_test, y_pred)\n",
    "    perf_metric_per_model['Accuracy_Score'] = accuracy_score(y_test, y_pred)\n",
    "    perf_metric_per_model['Precision'] = precision_score(y_test, y_pred)\n",
    "    perf_metric_per_model['Recall'] = recall_score(y_test, y_pred) \n",
    "    perf_metric_per_model['Train_score'] = model.score(X_train, y_train)    \n",
    "    perf_metric_per_model['Test_score'] = model.score(X_test, y_test)    \n",
    "    perf_metric_per_model['Matt_Coeff'] = matthews_corrcoef(y_test, y_pred)\n",
    "    s2 = pd.Series(perf_metric_per_model, index=perf_metric_per_model.keys())\n",
    "    df_metric = pd.DataFrame(data=[s2], columns=perf_metrics)\n",
    "    add_perf_metric_to_compare(perf_metric_per_model)\n",
    "    return df_metric\n",
    "    \n",
    "# plot feature importances\n",
    "# not applicable to Logistic Regression\n",
    "def plot_feature_importance(model_name, importances, columns):\n",
    "    # feature importances\n",
    "    plt.figure(figsize=(12,6))\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    indices = indices[:10]\n",
    "    plt.title('Top 10 Feature Importances by {}'.format(model_name))\n",
    "    plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "    plt.yticks(range(len(indices)), columns[indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()\n",
    "\n",
    "# plot ROC and Precision, Recall curve\n",
    "def plot_roc_and_precision(model_name, model, X_test, y_test):\n",
    "    # calculate the fpr and tpr for all thresholds of the classification\n",
    "    probs = model.predict_proba(X_test)\n",
    "    preds = probs[:,1]\n",
    "    fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # AUC under ROC Curve \n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.subplot(121)\n",
    "    plt.title('Receiver Operating Characteristic for {}'.format(model_name))\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('TPR (Recall or Sensitivity)')\n",
    "    plt.xlabel('FPR (1-Specificity)')\n",
    "    \n",
    "    # precision and recall curve\n",
    "    plt.subplot(122)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
    "    plt.step(recall, precision, color='b', alpha=0.1,\n",
    "         where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall curve for {}'.format(model_name))\n",
    "    \n",
    "    plt.tight_layout(pad=4.0, w_pad=0.5, h_pad=1.0)    \n",
    "    plt.show()\n",
    "\n",
    "metrics = pd.DataFrame(index=['accuracy','precision','recall','auc_roc_score'],columns=['LogR','DTree','RF'])\n",
    "# metrics    \n",
    "def CMatrix(CM,labels=['pay','default']):\n",
    "    df1 = pd.DataFrame(data=CM,index=labels,columns=labels)\n",
    "    df1.index.name = 'Actual'\n",
    "    df1.columns.name = 'Predicted'\n",
    "    df1.loc['Total'] = df1.sum()\n",
    "    df1['Total'] = df1.sum(axis=1)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information related to Credit Limit, Payment Status, Bill Amount and Bill Payment\n",
    "We will look at how payment status, bill amount and bill payment columns look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cnt = df.shape[0]\n",
    "print 'Class Statistics on entire data {}:'.format(total_cnt)\n",
    "cls_cnt = df['default_pay'].value_counts()\n",
    "cls_stats = pd.DataFrame ({\"Count\": [cls_cnt[0], cls_cnt[1]],\\\n",
    "                        \"Percent Split\" : [((cls_cnt[0] * 100.0) /total_cnt), ((cls_cnt[1] * 100.0) /total_cnt)]},\n",
    "                         index=[\"Not Default\", \"Default\"])\n",
    "display(cls_stats)\n",
    "plt.figure(figsize=(5,5))\n",
    "ax = cls_cnt.plot(kind='bar')\n",
    "ax.set_xlabel(\"Default Status\")\n",
    "ax.set_ylabel(\"No. of Instances\")\n",
    "ax.set_xticklabels(['Not Default', 'Default'])\n",
    "#     for p in ax.patches:\n",
    "#         ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+300))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information related to Credit Limit, Payment Status, Bill Amount and Bill Payment\n",
    "We will look at how payment status, bill amount and bill payment columns look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[pay_status_columns].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Paid Amount Columns'\n",
    "display(df[pay_amt_columns].head(10))\n",
    "print ('==')*10\n",
    "print '\\nPaid Amount Columns Statistics'\n",
    "display(df[pay_amt_columns].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Bill Amount Columns'\n",
    "display(df[bill_amt_columns].head(10))\n",
    "print ('==')*10\n",
    "print '\\nBill Amount Columns Statistics'\n",
    "display(df[bill_amt_columns].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['limit_bal'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_t = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pay = pd.DataFrame(columns=['credit_assigned', 'total_bill','average_bill', 'total_payment', \\\n",
    "                               'average_payment', 'amt_owed','amt_owed_more_thn_credit','default_status',\n",
    "                              'adv_pay', 'min_pay', 'delay_pay'])\n",
    "df_pay['credit_assigned'] = df['limit_bal']\n",
    "df_pay['total_bill'] = df[bill_amt_columns].sum(axis=1)\n",
    "df_pay['average_bill'] = df[bill_amt_columns].mean(axis=1)\n",
    "df_pay['total_payment'] = df[pay_amt_columns].sum(axis=1)\n",
    "df_pay['average_payment'] = df[pay_amt_columns].mean(axis=1)\n",
    "df_pay['amt_owed'] = df_pay['total_bill'] - df_pay['total_payment']\n",
    "\n",
    "\n",
    "df_pay['amt_owed_more_thn_credit'] = np.where(\n",
    "    (df_pay['credit_assigned'] - df_pay['amt_owed']) > 0,\\\n",
    "    0, 1)\n",
    "\n",
    "df_5 = df[pay_status_columns]\n",
    "df_pay['adv_pay'] = df_5[df_5 < 0 ].count(axis=1)\n",
    "df_pay['min_pay'] = df_5[df_5 == 0 ].count(axis=1)\n",
    "df_pay['delay_pay'] = df_5[df_5 > 0 ].count(axis=1)\n",
    "df_pay['default_status'] = df['default_pay']\n",
    "display(df_pay.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exrracting more information from the dataset for new feature creation \n",
    "- _adv pay_ : Number of times -2,-1 has occured in pay status\n",
    "- _min Pay_ : Number of times 0 has occured in pay status\n",
    "- _delay pay_: Number of times 1 and above digits occured in pay status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3)\n",
    "fig.set_size_inches(15,5)\n",
    "fig.suptitle('Distribution of dalays in the past 6 months')\n",
    "\n",
    "for i in range(len(pay_status_columns)):\n",
    "    row,col = int(i/3), i%3\n",
    "\n",
    "    d  = df[pay_status_columns[i]].value_counts()\n",
    "    ax[row,col].bar(d.index, d, align='center', color='g')\n",
    "    ax[row,col].set_title(pay_status_columns[i])\n",
    "\n",
    "plt.tight_layout(pad=3.0, w_pad=0.5, h_pad=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3)\n",
    "fig.set_size_inches(18,9)\n",
    "fig.suptitle('Distribution of dalays repayments in the past 6 months')\n",
    "for i in range(len(pay_status_columns)):\n",
    "    row,col = int(i/3), i%3\n",
    "    sns.countplot(x=pay_status_columns[i], hue=\"default_pay\", data=df, ax=ax[row,col])\n",
    "    ax[row,col].set_title(pay_status_columns[i])\n",
    "\n",
    "plt.tight_layout(pad=3.0, w_pad=0.5, h_pad=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['pay_1','pay_2','pay_3','pay_4','pay_5','pay_6','sex','education','marriage','default_pay']\n",
    "df_tmp = df.drop(labels=cols_to_drop,axis=1)\n",
    "df_tmp.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "lst = list(pay_amt_columns)\n",
    "lst.extend(bill_amt_columns)\n",
    "lst.extend(['limit_bal','age','default_pay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df.copy()\n",
    "df_corr['default_pay'] = df_original['default payment next month']\n",
    "#correlation matrix\n",
    "cm = df_corr.corr()\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(cm, annot=True, cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Limit Balance__ is negatively correlated with target variable.\n",
    "- __bill amt__ features are internally strongly positively correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Marriage:  ', pd.unique(df['marriage'])\n",
    "print 'Education: ', pd.unique(df['education'])\n",
    "print 'Sex:       ', pd.unique(df['sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per documentation of the dataset, for education, we only have following<br/>\n",
    "1 = graduate school; 2 = university; 3 = high school; 4 = others<br/>\n",
    "We will replace 0,5,6 with 'others'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df['sex'] = df['sex'].astype('category').cat.rename_categories(['M', 'F'])\n",
    "df['marriage'] = df['marriage'].astype('category').cat.rename_categories(['na','married', 'single', 'other'])\n",
    "df['education'] = df['education'].astype('category').cat.rename_categories(['na','grad_school','university', 'high_school', 'others','unknown1','unknown2'])\n",
    "df['age_cat'] = pd.cut(df['age'], range(0, 100, 10), right=False)\n",
    "for i in pay_status_columns:\n",
    "    df[i] = df[i].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,4)\n",
    "fig.set_size_inches(16,4)\n",
    "fig.suptitle('Defaulting by absolute numbers, for various demographics')\n",
    "\n",
    "d = df.groupby(['default_pay', 'sex']).size()\n",
    "p = d.unstack(level=1).plot(kind='bar', ax=ax[0])\n",
    "\n",
    "d = df.groupby(['default_pay', 'marriage']).size()\n",
    "p = d.unstack(level=1).plot(kind='bar', ax=ax[1])\n",
    "\n",
    "d = df.groupby(['default_pay', 'education']).size()\n",
    "p = d.unstack(level=1).plot(kind='bar', ax=ax[2])\n",
    "\n",
    "d = df.groupby(['default_pay', 'age_cat']).size()\n",
    "p = d.unstack(level=1).plot(kind='bar', ax=ax[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,4)\n",
    "fig.set_size_inches(16,4)\n",
    "fig.suptitle('Defaulting by relative numbers given each class, for various demographics')\n",
    "\n",
    "d = df.groupby(['default_pay', 'sex']).size().unstack(level=1)\n",
    "d = d / d.sum()\n",
    "p = d.plot(kind='bar', ax=ax[0])\n",
    "\n",
    "d = df.groupby(['default_pay', 'marriage']).size().unstack(level=1)\n",
    "d = d / d.sum()\n",
    "p = d.plot(kind='bar', ax=ax[1])\n",
    "\n",
    "d = df.groupby(['default_pay', 'education']).size().unstack(level=1)\n",
    "d = d / d.sum()\n",
    "p = d.plot(kind='bar', ax=ax[2])\n",
    "\n",
    "d = df.groupby(['default_pay', 'age_cat']).size().unstack(level=1)\n",
    "d = d / d.sum()\n",
    "p = d.plot(kind='bar', ax=ax[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above graph, \n",
    "+ Distribution of Gender: \n",
    "    - Female credit card holders are more compared to Male\n",
    "    - Female defaulters are more compared to Male\n",
    "+ Distribution of Marriage: \n",
    "    - Single are holding more credit cards compared to Married and Others\n",
    "    - Within defaulters Marital status with Others is very minimal. Married and Single have same number of defaulters\n",
    "+ Distribution of Education:\n",
    "    - University education has more credit crds. People with education values as Graduate and University have more credit cards.   \n",
    "+ Distribution by Age:\n",
    "    - Age 30 to 40 have more credit cards followed by Age 20 to 30.\n",
    "    - Age 60 to 70 and Age 70 to 80 have very less credit card holders\n",
    "    - Age 20 to 30 and 30 to 40 have more defaulters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit limit distribution\n",
    "sns.distplot(df['limit_bal'],kde=True)\n",
    "print 'Skewness on limit_bal is : ',skew(df['limit_bal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking outliers for bill amount[1 to 6]\n",
    "number_of_columns=6\n",
    "number_of_rows = len(bill_amt_columns)-1/number_of_columns\n",
    "plt.figure(figsize=(2*number_of_columns,5*number_of_rows))\n",
    "for i in range(0,len(bill_amt_columns)):\n",
    "    plt.subplot(number_of_rows + 1,number_of_columns,i+1)\n",
    "    sns.set_style('whitegrid')\n",
    "    sns.boxplot(df[bill_amt_columns[i]],color='green',orient='v')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking outliers for pay amount[1 to 6]\n",
    "number_of_columns=6\n",
    "number_of_rows = len(pay_amt_columns)-1/number_of_columns\n",
    "plt.figure(figsize=(2*number_of_columns,5*number_of_rows))\n",
    "for i in range(0,len(pay_amt_columns)):\n",
    "    plt.subplot(number_of_rows + 1,number_of_columns,i+1)\n",
    "    sns.set_style('whitegrid')\n",
    "    sns.boxplot(df[pay_amt_columns[i]],color='green',orient='v')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As it can be observed that data is infested with outliers(by the logic of extreme values beyond 1.5*IQR), but since removing them would cost lot of valuable loss of information.We decided not to treat them as ouliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking missing values\n",
    "sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')\n",
    "#Plane box indicates no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are no missing values in data, its a clean dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df['grad_school'] = (df['education']==1).astype('int')\n",
    "df['university'] = (df['education']==2).astype('int')\n",
    "df['high_school']= (df['education']==3).astype('int')\n",
    "df['others']= (df['education']==4).astype('int')\n",
    "df['others']= (df['education']==5).astype('int')\n",
    "df['others']= (df['education']==6).astype('int')\n",
    "df['others']= (df['education']==0).astype('int')\n",
    "df.drop('education',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['male'] = (df['sex']=='M').astype('int')\n",
    "df.drop('sex',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "df['married'] = (df['marriage']==1).astype('int')\n",
    "df['single'] = (df['marriage']==2).astype('int')\n",
    "df['na'] = (df['marriage']==3).astype('int')\n",
    "df['na'] = (df['marriage']==0).astype('int')\n",
    "df.drop(['marriage','age_cat'],axis=1,inplace=True)\n",
    "\n",
    "# Since 0 is labeled as 'pay duly', every negative value should be seen as 0.\n",
    "for i in pay_status_columns:\n",
    "    df[i].replace({-2:-1,-1:-1},inplace=True)\n",
    " \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Scaling\n",
    "scale = ['limit_bal','bill_amt1', 'bill_amt2', 'bill_amt3', 'bill_amt4', 'bill_amt5', 'bill_amt6','pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5', 'pay_amt6']\n",
    "robust_scaler = RobustScaler()\n",
    "df.loc[:,scale] = robust_scaler.fit_transform(df.loc[:,scale])\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_values = df.columns.values\n",
    "print type(col_values)\n",
    "out = np.delete(col_values, np.where(col_values == 'default'))\n",
    "col_values = out \n",
    "print col_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection and Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We tried different methods with aim to find improvements in prediction outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Method 1_  : RFE - wrapper method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=9)\n",
    "dt = DecisionTreeClassifier(random_state=9)\n",
    "rf = RandomForestClassifier(random_state=9)\n",
    "X = df.drop('default_pay',axis=1)\n",
    "y = df['default_pay']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=9,test_size=.2)\n",
    "# create the RFE model and select 10 attributes\n",
    "rfe1 = RFE(lr, 10)\n",
    "rfe1 = rfe1.fit(X, y)\n",
    "print ('Logistic Regression Feature Rankings:')\n",
    "print(rfe1.ranking_)\n",
    "print ('==')*40\n",
    "\n",
    "rfe2 = RFE(dt, 10)\n",
    "rfe2 = rfe2.fit(X, y)\n",
    "print ('Decision Tree Feature Rankings:')\n",
    "print(rfe2.ranking_)\n",
    "print ('==')*40\n",
    "\n",
    "rfe3 = RFE(rf, 10)\n",
    "rfe3 = rfe3.fit(X, y)\n",
    "print ('Random Forest Feature Rankings:')\n",
    "print(rfe2.ranking_)\n",
    "print ('==')*40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "# Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('RF2', RandomForestClassifier()))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f\" % (name, cv_results.mean())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logisitics regression performed better in kfold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting rfe1 features since lr performed good in KFold\n",
    "X = df.iloc[:,[0,2,3,4,6,10,8,9,14,24]]\n",
    "y = df.default_pay\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=9,test_size=.2)\n",
    "\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "model = lr\n",
    "model_name = 'ITR1: Logistic Regression (post RFE)'\n",
    "model_description = 'ITR1: Logistic Regression (with RFE)'\n",
    "df_metric = save_performance_metrics(model, model_name, model_description, X_test, y_test, X_train, y_train)\n",
    "# display(CMatrix(CM))\n",
    "# display(df_metric)\n",
    "# plot_roc_and_precision(model_name, model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=y_test,y_pred=y_pred))\n",
    "print 'mcc : {}'.format( matthews_corrcoef(y_true=y_test,y_pred=y_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(y_true=y_test,y_pred=y_pred).ravel()\n",
    "print(\"\\n TN: {} FP: {} FN: {} TP: {} \\n\".format(tn, fp, fn, tp))\n",
    "print 'auc: {}'.format(roc_auc_score(y_true=y_test,y_score=y_pred))\n",
    "print 'accuracy: {}'.format(accuracy_score(y_true=y_test,y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFE with RandomForest since there was just .01 difference between lr\n",
    "# create the RFE model and select 10 attributes\n",
    "X = df.iloc[:,[0,1,2,8,9,10,11,12,13,15]]\n",
    "y = df.default_pay\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=9,test_size=.2)\n",
    "rf.fit(X_train,y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print(classification_report(y_true=y_test,y_pred=y_pred))\n",
    "print 'mcc : {}'.format( matthews_corrcoef(y_true=y_test,y_pred=y_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(y_true=y_test,y_pred=y_pred).ravel()\n",
    "print(\"\\n TN: {} FP: {} FN: {} TP: {} \\n\".format(tn, fp, fn, tp))\n",
    "print 'auc: {}'.format(roc_auc_score(y_true=y_test,y_score=y_pred))\n",
    "print 'accuracy: {}'.format(accuracy_score(y_true=y_test,y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Stcaking Ensemble with RFE1\n",
    "X = df.iloc[:,[0,2,3,4,6,10,8,9,14,24]]\n",
    "y = df['default_pay']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=9)\n",
    "X_train1, X_test1, y_train2, y_test2 = train_test_split(X_train, y_train, test_size=0.2, random_state=9)\n",
    "\n",
    "M1 = LogisticRegression()\n",
    "M2 = DecisionTreeClassifier(max_depth=1,criterion='entropy')\n",
    "M3 = DecisionTreeClassifier(max_depth=2,criterion='entropy')\n",
    "M4 = DecisionTreeClassifier(max_depth=1,criterion='entropy')\n",
    "M5 = DecisionTreeClassifier(max_depth=1,criterion='entropy')\n",
    "\n",
    "\n",
    "\n",
    "M1.fit(X_train1,y_train2)\n",
    "M2.fit(X_train1,y_train2)\n",
    "M3.fit(X_train1,y_train2)\n",
    "M4.fit(X_train1,y_train2)\n",
    "M5.fit(X_train1,y_train2)\n",
    "\n",
    "\n",
    "pred1 = M1.predict_proba(X_test1)\n",
    "pred2 = M2.predict_proba(X_test1)\n",
    "pred3 = M3.predict_proba(X_test1)\n",
    "pred4 = M4.predict_proba(X_test1)\n",
    "pred5 = M5.predict_proba(X_test1)\n",
    "\n",
    "\n",
    "A = pd.DataFrame(pred1)[1]\n",
    "B = pd.DataFrame(pred2)[1]\n",
    "C = pd.DataFrame(pred3)[1]\n",
    "D = pd.DataFrame(pred4)[1]\n",
    "E = pd.DataFrame(pred5)[1]\n",
    "\n",
    "X_meta = pd.concat([A,B,C,D,E],axis=1)\n",
    "\n",
    "meta_classifier = RandomForestClassifier(max_depth=2,n_estimators=100)\n",
    "meta_classifier.fit(X_meta,y_test2)\n",
    "\n",
    "\n",
    "pred6 = M1.predict_proba(X_test)\n",
    "pred7 = M2.predict_proba(X_test)\n",
    "pred8 = M3.predict_proba(X_test)\n",
    "pred9 = M4.predict_proba(X_test)\n",
    "pred10 = M5.predict_proba(X_test)\n",
    "\n",
    "\n",
    "F = pd.DataFrame(pred6)[1]\n",
    "G = pd.DataFrame(pred7)[1]\n",
    "H = pd.DataFrame(pred8)[1]\n",
    "I = pd.DataFrame(pred9)[1]\n",
    "J = pd.DataFrame(pred10)[1]\n",
    "\n",
    "K = pd.concat([F,G,H,I,J], axis=1)\n",
    "y_meta = meta_classifier.predict(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print classification_report(y_meta,y_test)\n",
    "print \"auc:{}\".format(roc_auc_score(y_meta,y_test)) \n",
    "print \"mcc: {}\".format(matthews_corrcoef(y_meta,y_test))\n",
    "tn, fp, fn, tp = confusion_matrix(y_meta,y_test).ravel()\n",
    "print (\"\\n TN: {} FP: {} FN: {} TP: {} \\n\".format(tn, fp, fn, tp))\n",
    "print \"accuracy: {}\".format(accuracy_score(y_meta,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Stcaking Ensemble with RFE3\n",
    "X = df.iloc[:,[0,2,1,10,8,9,11,12,13,15]]\n",
    "y = df['default_pay']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=9)\n",
    "X_train1, X_test1, y_train2, y_test2 = train_test_split(X_train, y_train, test_size=0.2, random_state=9)\n",
    "\n",
    "M1 = LogisticRegression()\n",
    "M2 = DecisionTreeClassifier(max_depth=1,criterion='entropy')\n",
    "M3 = DecisionTreeClassifier(max_depth=2,criterion='entropy')\n",
    "M4 = DecisionTreeClassifier(max_depth=1,criterion='entropy')\n",
    "M5 = DecisionTreeClassifier(max_depth=1,criterion='entropy')\n",
    "\n",
    "\n",
    "\n",
    "M1.fit(X_train1,y_train2)\n",
    "M2.fit(X_train1,y_train2)\n",
    "M3.fit(X_train1,y_train2)\n",
    "M4.fit(X_train1,y_train2)\n",
    "M5.fit(X_train1,y_train2)\n",
    "\n",
    "\n",
    "pred1 = M1.predict_proba(X_test1)\n",
    "pred2 = M2.predict_proba(X_test1)\n",
    "pred3 = M3.predict_proba(X_test1)\n",
    "pred4 = M4.predict_proba(X_test1)\n",
    "pred5 = M5.predict_proba(X_test1)\n",
    "\n",
    "\n",
    "A = pd.DataFrame(pred1)[1]\n",
    "B = pd.DataFrame(pred2)[1]\n",
    "C = pd.DataFrame(pred3)[1]\n",
    "D = pd.DataFrame(pred4)[1]\n",
    "E = pd.DataFrame(pred5)[1]\n",
    "\n",
    "X_meta = pd.concat([A,B,C,D,E],axis=1)\n",
    "\n",
    "meta_classifier = RandomForestClassifier(max_depth=2,n_estimators=100)\n",
    "meta_classifier.fit(X_meta,y_test2)\n",
    "\n",
    "\n",
    "pred6 = M1.predict_proba(X_test)\n",
    "pred7 = M2.predict_proba(X_test)\n",
    "pred8 = M3.predict_proba(X_test)\n",
    "pred9 = M4.predict_proba(X_test)\n",
    "pred10 = M5.predict_proba(X_test)\n",
    "\n",
    "\n",
    "F = pd.DataFrame(pred6)[1]\n",
    "G = pd.DataFrame(pred7)[1]\n",
    "H = pd.DataFrame(pred8)[1]\n",
    "I = pd.DataFrame(pred9)[1]\n",
    "J = pd.DataFrame(pred10)[1]\n",
    "\n",
    "K = pd.concat([F,G,H,I,J], axis=1)\n",
    "y_meta = meta_classifier.predict(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print classification_report(y_meta,y_test)\n",
    "print \"auc:{}\".format(roc_auc_score(y_meta,y_test)) \n",
    "print \"mcc: {}\".format(matthews_corrcoef(y_meta,y_test))\n",
    "tn, fp, fn, tp = confusion_matrix(y_meta,y_test).ravel()\n",
    "print (\"\\n TN: {} FP: {} FN: {} TP: {} \\n\".format(tn, fp, fn, tp))\n",
    "print \"accuracy: {}\".format(accuracy_score(y_meta,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Method 2: PCA_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_selection import chi2,f_classif,SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "#converting dataframe into numpy Array\n",
    "X = df.drop('default_pay',axis=1)\n",
    "Y = df.default_pay\n",
    "\n",
    "# Split-out validation dataset\n",
    "validation_size = 0.20\n",
    "seed = 7\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the values\n",
    "X_t = scale(X_train)\n",
    "\n",
    "#initially lets create 28 components which is actual number of Variables we have\n",
    "pca = PCA(n_components=28)\n",
    "\n",
    "pca.fit(X_t)\n",
    "\n",
    "#The amount of variance that each PC explains\n",
    "var= pca.explained_variance_ratio_\n",
    "\n",
    "#Cumulative Variance explains\n",
    "var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "#lets see Cumulative Variance plot\n",
    "plt.plot(var1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Looking at above plot I'm taking 16 variables\n",
    "pca = PCA(n_components=16)\n",
    "pca.fit(X_t)\n",
    "X_train_PC=pca.fit_transform(X_t)\n",
    "# X_train_PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(random_state=9)))\n",
    "models.append(('RF2', RandomForestClassifier(random_state=9)))\n",
    "models.append(('DT', DecisionTreeClassifier(random_state=9)))\n",
    "\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "# Test options and evaluation metric\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = cross_val_score(model, X_train_PC, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f\" % (name, cv_results.mean())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing Algorithms\n",
    "fig = plt.figure(figsize=(5,2))\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "\n",
    "#Scaling the X_validation data\n",
    "X_v = scale(X_validation)\n",
    "\n",
    "pca.fit(X_v)\n",
    "X_validation_PC=pca.fit_transform(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on validation dataset by choosing best Algorithm\n",
    "lr = LogisticRegression(random_state=9)\n",
    "lr.fit(X_train_PC, Y_train)\n",
    "predictions = lr.predict(X_validation_PC)\n",
    "print \"accuracy: {}\".format(accuracy_score(Y_validation, predictions))\n",
    "tn, fp, fn, tp = confusion_matrix(Y_validation, predictions).ravel()\n",
    "print (\"\\n TN: {} FP: {} FN: {} TP: {} \\n\".format(tn, fp, fn, tp))\n",
    "print(classification_report(Y_validation, predictions))\n",
    "print \"mcc: {}\".format(matthews_corrcoef(Y_validation, predictions))\n",
    "print 'auc: {}'.format(roc_auc_score(Y_validation, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Method  3 : Select From Model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X = df.drop('default_pay',axis=1)\n",
    "y = df.default_pay\n",
    "print('Before');\n",
    "print(X.shape)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X, y)\n",
    "\n",
    "print('\\nFeature Importance');\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "\n",
    "print('\\nAfter'); \n",
    "print(X_new.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.20, random_state=42)\n",
    "smote = SMOTE(random_state=9, kind=\"borderline2\")               # With OverSampling Technique considering data is imbalance for a moment \n",
    "X_sample, y_sample = smote.fit_sample(X_train, y_train)\n",
    "clf.fit(X_sample, y_sample)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_true=y_test,y_pred=y_pred))\n",
    "print 'mcc : {}'.format( matthews_corrcoef(y_true=y_test,y_pred=y_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(y_true=y_test,y_pred=y_pred).ravel()\n",
    "print(\"\\n TN: {} FP: {} FN: {} TP: {} \\n\".format(tn, fp, fn, tp))\n",
    "print 'auc: {}'.format(roc_auc_score(y_true=y_test,y_score=y_pred))\n",
    "print 'accuracy: {}'.format(accuracy_score(y_true=y_test,y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Method 4: SelectKBest_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogR = LogisticRegression(random_state=10)\n",
    "LogR.fit(X_train, y_train)\n",
    "y_pred_test = LogR.predict(X_test)\n",
    "metrics.loc['accuracy','LogR'] = accuracy_score(y_pred=y_pred_test,y_true=y_test)\n",
    "metrics.loc['precision','LogR'] = precision_score(y_pred=y_pred_test,y_true=y_test)\n",
    "metrics.loc['recall','LogR'] = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "metrics.loc['auc_roc_score','LogR'] = roc_auc_score(y_score=y_pred_test,y_true=y_test)\n",
    "CM = confusion_matrix(y_pred=y_pred_test,y_true=y_test)\n",
    "model = LogR\n",
    "model_name = 'ITR2(KBest,K=25): Simple Logistic Regression'\n",
    "model_description = 'ITR2(KBest,K=25): basic Logistic Regression applied'\n",
    "df_metric = save_performance_metrics(model, model_name, model_description, X_test, y_test, X_train, y_train)\n",
    "display(CMatrix(CM))\n",
    "display(df_metric)\n",
    "plot_roc_and_precision(model_name, model, X_test, y_test)\n",
    "# display(df_all_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTree = DecisionTreeClassifier(max_leaf_nodes=10,min_samples_split=30)\n",
    "DTree.fit(X_train, y_train)\n",
    "# print len(X_train[0])\n",
    "y_pred_test = DTree.predict(X_test)\n",
    "metrics.loc['accuracy','DTree'] = accuracy_score(y_pred=y_pred_test,y_true=y_test)\n",
    "metrics.loc['precision','DTree'] = precision_score(y_pred=y_pred_test,y_true=y_test)\n",
    "metrics.loc['recall','DTree'] = recall_score(y_pred=y_pred_test,y_true=y_test)\n",
    "metrics.loc['auc_roc_score','DTree'] = roc_auc_score(y_score=y_pred_test,y_true=y_test)\n",
    "CM = confusion_matrix(y_pred=y_pred_test,y_true=y_test)\n",
    "model = DTree\n",
    "model_name = 'ITR2(KBest,K=25): Decision Tree'\n",
    "model_description = 'ITR2(KBest,K=25): Decision Tree (with max_leaf_node=10, sample_split=30)'\n",
    "df_metric = save_performance_metrics(model, model_name, model_description, X_test, y_test, X_train, y_train)\n",
    "display(CMatrix(CM))\n",
    "display(df_metric)\n",
    "plot_roc_and_precision(model_name, model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Method 5: XgBoost_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#converting dataframe into numpy Array\n",
    "X = df.drop('default_pay',axis=1)\n",
    "Y = df.default_pay\n",
    "\n",
    "# Split-out validation dataset\n",
    "validation_size = 0.20\n",
    "seed = 7\n",
    "X_trainB, X_testB, Y_trainB, Y_testB = train_test_split(X, Y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model on training data\n",
    "XGBoost = XGBClassifier()\n",
    "XGBoost.fit(X_trainB, Y_trainB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# make predictions for test data\n",
    "y_predB = XGBoost.predict(X_testB)\n",
    "predictions = [round(value) for value in y_predB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate predictions\n",
    "print(classification_report(y_true=Y_testB,y_pred=y_predB))\n",
    "print 'mcc : {}'.format( matthews_corrcoef(y_true=Y_testB,y_pred=y_predB))\n",
    "tn, fp, fn, tp = confusion_matrix(y_true=Y_testB,y_pred=y_predB).ravel()\n",
    "print(\"\\n TN: {} FP: {} FN: {} TP: {} \\n\".format(tn, fp, fn, tp))\n",
    "print 'auc: {}'.format(roc_auc_score(y_true=Y_testB,y_score=y_predB))\n",
    "print 'accuracy: {}'.format(accuracy_score(y_true=Y_testB,y_pred=y_predB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance using built-in function\n",
    "from xgboost import plot_importance\n",
    "plot_importance(XGBoost)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other approach - Month-wise models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "pd.unique(df_sample['pay_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for i in pay_status_columns:\n",
    "    df_sample[i].replace({-2:0,-1:0,2:1,3:1,4:1,5:1,6:1,7:1,8:1,9:1},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X, y,random_state = 9,train_size = 0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=4)\n",
    "clf_gini.fit(X_train,y_train)\n",
    "\n",
    "y_pred = clf_gini.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "z = confusion_matrix(y_test, y_pred)\n",
    "y = classification_report(y_test, y_pred)\n",
    "train_score, test_score = clf_gini.score(X_train,y_train), clf_gini.score(X_test, y_test)\n",
    "print train_score, test_score\n",
    "print y, z\n",
    "#print roc_auc_score(y_true=y_10,y_score=y_pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_sample_90, df_sample_10 = train_test_split(df,random_state = 9,train_size = 0.90)\n",
    "y_train, y_test = train_test_split(df_sample_90['default_pay'],random_state = 9,train_size = 0.80)\n",
    "y_10 = df_sample_10['default_pay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_90 = df_sample_90\n",
    "df_10 = df_sample_10\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X1=df_90[['limit_bal','education','age','sex','bill_amt1','pay_amt1']]\n",
    "y1=df_90['pay_1']\n",
    "X2=df_90[['limit_bal','education','age','sex','bill_amt2','pay_amt2']]\n",
    "y2=df_90['pay_2']\n",
    "X3=df_90[['limit_bal','education','age','sex','bill_amt3','pay_amt3']]\n",
    "y3=df_90['pay_3']\n",
    "X4=df_90[['limit_bal','education','age','sex','bill_amt4','pay_amt4']]\n",
    "y4=df_90['pay_4']\n",
    "X5=df_90[['limit_bal','education','age','sex','bill_amt5','pay_amt5']]\n",
    "y5=df_90['pay_5']\n",
    "X6=df_90[['limit_bal','education','age','sex','bill_amt6','pay_amt6']]\n",
    "y6=df_90['pay_6']\n",
    "X1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rfe = RFE(X2_dt)\n",
    "rfe = rfe.fit(X2, y2)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X1=df_90[['limit_bal','bill_amt1','pay_amt1']]\n",
    "y1=df_90['pay_1']\n",
    "X2=df_90[['limit_bal','bill_amt2','pay_amt2']]\n",
    "y2=df_90['pay_2']\n",
    "X3=df_90[['limit_bal','bill_amt3','pay_amt3']]\n",
    "y3=df_90['pay_3']\n",
    "X4=df_90[['limit_bal','bill_amt4','pay_amt4']]\n",
    "y4=df_90['pay_4']\n",
    "X5=df_90[['limit_bal','bill_amt5','pay_amt5']]\n",
    "y5=df_90['pay_5']\n",
    "X6=df_90[['limit_bal','bill_amt6','pay_amt6']]\n",
    "y6=df_90['pay_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X1_train,X1_test,X2_train,X2_test,X3_train,X3_test,X4_train,X4_test,X5_train,X5_test,X6_train,X6_test,y1_train,y1_test,y2_train,y2_test,y3_train,y3_test,y4_train,y4_test,y5_train,y5_test,y6_train,y6_test = train_test_split(X1,X2,X3,X4,X5,X6,y1,y2,y3,y4,y5,y6,random_state = 9,train_size = 0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "X1_dt = DecisionTreeClassifier(criterion='gini',max_depth=4)\n",
    "kfold = KFold(n_splits=5)\n",
    "cv_results = cross_val_score(X1_dt, X1_train, y1_train, cv=kfold, scoring='accuracy')\n",
    "print cv_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X1_dt.fit(X1_train,y1_train)\n",
    "\n",
    "y_pred1 = X1_dt.predict(X1_test)\n",
    "y_pred1\n",
    "a = confusion_matrix(y1_test, y_pred1)\n",
    "b = classification_report(y1_test, y_pred1)\n",
    "train_score, test_score = X1_dt.score(X1_train, y1_train), X1_dt.score(X1_test, y1_test)\n",
    "print b, train_score, test_score\n",
    "print y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5)\n",
    "X1_lr = LogisticRegression()\n",
    "\n",
    "cv_results = cross_val_score(X1_lr, X1_train, y1_train, cv=kfold, scoring='accuracy')\n",
    "print cv_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X1_lr.fit(X1_train,y1_train)\n",
    "\n",
    "y_pred1 = X1_lr.predict(X1_test)\n",
    "y_pred1\n",
    "a = confusion_matrix(y1_test, y_pred1)\n",
    "b = classification_report(y1_test, y_pred1)\n",
    "train_score, test_score = X1_dt.score(X1_train, y1_train), X1_dt.score(X1_test, y1_test)\n",
    "print b, train_score, test_score\n",
    "print y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5)\n",
    "X1_rf = RandomForestClassifier()\n",
    "\n",
    "cv_results = cross_val_score(X1_rf, X1_train, y1_train, cv=kfold, scoring='accuracy')\n",
    "print cv_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X1_rf.fit(X1_train,y1_train)\n",
    "\n",
    "y_pred1 = X1_rf.predict(X1_test)\n",
    "y_pred1\n",
    "a = confusion_matrix(y1_test, y_pred1)\n",
    "b = classification_report(y1_test, y_pred1)\n",
    "train_score, test_score = X1_dt.score(X1_train, y1_train), X1_dt.score(X1_test, y1_test)\n",
    "print b, train_score, test_score\n",
    "print y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "X2_dt = DecisionTreeClassifier(criterion='gini',max_depth=4)\n",
    "kfold = KFold(n_splits=5)\n",
    "cv_results = cross_val_score(X2_dt, X2_train, y2_train, cv=kfold, scoring='accuracy')\n",
    "print cv_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X2_dt.fit(X2_train,y2_train)\n",
    "\n",
    "y_pred2 = X2_dt.predict(X2_test)\n",
    "y_pred2\n",
    "c = confusion_matrix(y2_test, y_pred1)\n",
    "d = classification_report(y2_test, y_pred2)\n",
    "train_score, test_score = X2_dt.score(X2_train, y2_train), X2_dt.score(X2_test, y2_test)\n",
    "print d, train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X3_dt = DecisionTreeClassifier(criterion='gini',max_depth=4)\n",
    "kfold = KFold(n_splits=5)\n",
    "cv_results = cross_val_score(X3_dt, X3_train, y3_train, cv=kfold, scoring='accuracy')\n",
    "print cv_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X3_dt = DecisionTreeClassifier(criterion='gini',max_depth=4)\n",
    "X3_dt.fit(X3_train,y3_train)\n",
    "\n",
    "y_pred3 = X3_dt.predict(X3_test)\n",
    "y_pred3\n",
    "e = confusion_matrix(y3_test, y_pred3)\n",
    "f = classification_report(y3_test, y_pred3)\n",
    "train_score, test_score = X3_dt.score(X3_train, y3_train), X3_dt.score(X3_test, y3_test)\n",
    "print f, train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X4_dt = DecisionTreeClassifier(criterion='gini',max_depth=4)\n",
    "kfold = KFold(n_splits=5)\n",
    "cv_results = cross_val_score(X4_dt, X4_train, y4_train, cv=kfold, scoring='accuracy')\n",
    "print cv_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#X4_dt = DecisionTreeClassifier(criterion='gini',max_depth=4)\n",
    "X4_dt.fit(X4_train,y4_train)\n",
    "\n",
    "y_pred4 = X4_dt.predict(X4_test)\n",
    "y_pred4\n",
    "g = confusion_matrix(y4_test, y_pred4)\n",
    "h = classification_report(y4_test, y_pred4)\n",
    "train_score, test_score = X4_dt.score(X4_train, y4_train), X4_dt.score(X4_test, y4_test)\n",
    "print h, train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X5_dt = DecisionTreeClassifier(criterion='gini',max_depth=4)\n",
    "kfold = KFold(n_splits=5)\n",
    "cv_results = cross_val_score(X5_dt, X5_train, y5_train, cv=kfold, scoring='accuracy')\n",
    "print cv_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#X5_dt = DecisionTreeClassifier(criterion='gini',max_depth=4)\n",
    "X5_dt.fit(X5_train,y5_train)\n",
    "\n",
    "y_pred5 = X5_dt.predict(X5_test)\n",
    "y_pred5\n",
    "i = confusion_matrix(y5_test, y_pred5)\n",
    "j = classification_report(y5_test, y_pred5)\n",
    "train_score, test_score = X5_dt.score(X5_train, y5_train), X5_dt.score(X5_test, y5_test)\n",
    "print j, train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X6_dt = DecisionTreeClassifier(criterion='gini',max_depth=4)\n",
    "kfold = KFold(n_splits=5)\n",
    "cv_results = cross_val_score(X6_dt, X6_train, y6_train, cv=kfold, scoring='accuracy')\n",
    "print cv_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#X6_dt = DecisionTreeClassifier(criterion='gini',max_depth=4)\n",
    "X6_dt.fit(X6_train,y6_train)\n",
    "\n",
    "y_pred6 = X6_dt.predict(X6_test)\n",
    "y_pred6\n",
    "k = confusion_matrix(y6_test, y_pred6)\n",
    "l = classification_report(y6_test, y_pred6)\n",
    "train_score, test_score = X6_dt.score(X6_train, y6_train), X6_dt.score(X6_test, y6_test)\n",
    "print l, train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame(list(zip(y_pred1,y_pred2,y_pred3,y_pred4,y_pred5,y_pred6)),columns=['p1','p2','p3','p4','p5','p6'])\n",
    "y_pred.head()\n",
    "y_test.reshape(-1,1)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(y_pred,y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X1_10=df_10[['limit_bal','bill_amt1','pay_amt1']]\n",
    "y1_10=df_10['pay_1']\n",
    "X2_10=df_10[['limit_bal','bill_amt2','pay_amt2']]\n",
    "y2_10=df_10['pay_2']\n",
    "X3_10=df_10[['limit_bal','bill_amt3','pay_amt3']]\n",
    "y3_10=df_10['pay_3']\n",
    "X4_10=df_10[['limit_bal','bill_amt4','pay_amt4']]\n",
    "y4_10=df_10['pay_4']\n",
    "X5_10=df_10[['limit_bal','bill_amt5','pay_amt5']]\n",
    "y5_10=df_10['pay_5']\n",
    "X6_10=df_10[['limit_bal','bill_amt6','pay_amt6']]\n",
    "y6_10=df_10['pay_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_10_pred1 = X1_dt.predict(X1_10)\n",
    "y_10_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_10_pred2 = X2_dt.predict(X2_10)\n",
    "y_10_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_10_pred3 = X3_dt.predict(X3_10)\n",
    "y_10_pred3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_10_pred4 = X4_dt.predict(X4_10)\n",
    "y_10_pred4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_10_pred5 = X5_dt.predict(X5_10)\n",
    "y_10_pred5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_10_pred6 = X6_dt.predict(X6_10)\n",
    "y_10_pred6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_pred_10 = pd.DataFrame(list(zip(y_10_pred1,y_10_pred2,y_10_pred3,y_10_pred4,y_10_pred5,y_10_pred6)),columns=['p10_1','p10_2','p10_3','p10_4','p10_5','p10_6'])\n",
    "y_pred_10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_pred_final.reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_pred_final = lr.predict(y_pred_10)\n",
    "y_pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_10.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "m = confusion_matrix(y_10, y_pred_final)\n",
    "n = classification_report(y_10, y_pred_final)\n",
    "train_score, test_score = lr.score(y_pred,y_test), lr.score(y_pred_10, y_10)\n",
    "print train_score, test_score\n",
    "print n\n",
    "print roc_auc_score(y_true=y_10,y_score=y_pred_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
